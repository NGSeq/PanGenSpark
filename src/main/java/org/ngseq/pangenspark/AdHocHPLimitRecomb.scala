/*
  * Copyright 2020 Altti Maarala
  * Licensed under the Apache License, Version 2.0 (the "License");
  * you may not use this file except in compliance with the License.
  * You may obtain a copy of the License at
  *
  *       http://www.apache.org/licenses/LICENSE-2.0
  *
  * Unless required by applicable law or agreed to in writing, software
  * distributed under the License is distributed on an "AS IS" BASIS,
  * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
  * See the License for the specific language governing permissions and
  * limitations under the License.
*/
package org.ngseq.pangenspark

import java.io.IOException
import java.net.URI
import java.util.Calendar

import org.apache.hadoop.conf.Configuration
import org.apache.hadoop.fs.{FSDataOutputStream, FileSystem, Path}
import org.apache.spark.mllib.linalg.distributed.{BlockMatrix, CoordinateMatrix, MatrixEntry}
import org.apache.spark.sql.SparkSession
import org.apache.spark.sql.functions.input_file_name

import scala.collection.mutable.ArrayBuffer

/**
  * Calculates the heaviest path parallel. Current implementation requires pos and gap files generated by the PanVC pipeline
  *  local run commmand example
  * ./spark-2.1.0-bin-hadoop2.7/bin/spark-submit  --master "local[*]" --driver-memory 4G --class fi.aalto.ngs.seqspark.pangenomics.ScoreMatrix kesa/pangenomics/target/pangenomics-0.9-jar-with-dependencies.jar "/l/simple_tmp_output/sam_files/mapped_reads_to*.pos" "Downloads/PanVC/example_data/reference_pg_aligned.index/recombinant.n*.full" "Downloads/PanVC/example_data/reference_pg_aligned.index/recombinant.n*.gap_positions" 10
  *
  * spark-submit  --master local[*] --driver-memory 4G --class fi.aalto.ngs.seqspark.pangenomics.AdHocHPSpark /media/data/dev/PanVC/simple_tmp_output/sam_files/mapped_reads_to*.pos /media/data/dev/PanVC/example_data/reference_pg_aligned.index/recombinant.n*.full /media/data/dev/PanVC/example_data/reference_pg_aligned.index/recombinant.n*.gap_positions 4 4
  *
  * args: [pos_files] [pan_genome_files] [gap_files] [num_blocks]
  *
  * pos_files: mapped reads to pos
  * pan_genome_files: fasta files (separate) (generated by the PanVC pipeline)
  * gap_files: gap positions
  * num_blocks: scorematrix partitions
  *
  */

//sbt "runMain AdHocHPLimitRecomb 2 100 10000"

object AdHocHPLimitRecomb {

  def main(args: Array[String]) {

    val spark = SparkSession.builder.appName("AdHocHPLimitRecomb").master("local").getOrCreate()
    val posFolder = args(0)
    val panFolder = args(1)
    val gapFiles = args(2)
    val numBlocks = args(3).toInt
    val kp = args(4).toInt
    val out = args(5)

    //construct matrix from pos files
    import spark.implicits._
    //load position files and their names
    val positions = spark.read.text(posFolder).filter(x => !x.getString(0).isEmpty).select(input_file_name, $"value").as[(String,String)].rdd

    val numberPattern = "\\d{1,}".r

    // load gap information
    val gaps = spark.read.text(gapFiles).filter(x => !x.getString(0).isEmpty).select(input_file_name, $"value").as[(String,String)].rdd
      .map(x => (numberPattern.findAllIn(x._1).toArray.last.toInt,x._2.toInt))

    // we should be able to collect gap info to master
    val gapsC = gaps.collect()

    // create MatrixEntires and move them to correct position based on gap info
    val emptyEntries = positions.flatMap{x =>
      val pair = x._2.split(" ")
      val pos = pair.head.toLong
      val readLength = pair.last.toLong
      val ref = numberPattern.findAllIn(x._1).toArray.last
      val g = gapsC.filter(_._1==ref.toInt).map(_._2)

      //move position based on gap information
      def consecutive(pos: Long): Long = {
        val pre = g.filter(_ < pos).length
        if(g.contains(pos)) {
          var add = 1
          while(g.contains(pos+add)) {
            add += 1
          }
          pre + add + pos
        } else {
          pre + pos
        }
      }

      //add reads
      for{
        i <- 0 until readLength.toInt
      } yield new MatrixEntry(ref.toLong-1,consecutive(pos+i),1)
    }

    //find values for gap positions
    //find continuous sequences and add corresponding value for them
    def subseq(ar: Array[Int]): Array[(Int,Int)] = {
      var buf = ArrayBuffer[(Int,Int)]()
      var total = 0
      if(ar.length==1) {
        Array((ar(0),ar(0)+1))
      } else {
        var i = 1
        var buf2 = ArrayBuffer[Int]()
        buf2 += ar(i-1)
        while(i < ar.length) {
          if(ar(i) == ar(i-1)+1){
            buf2 += ar(i)
          } else {
            buf = buf ++ buf2.map((_,ar(i-1)+1+total))
            total += buf2.length
            buf2.clear()
            buf2 += ar(i)
          }
          i+=1
        }
        if(!buf2.isEmpty) {
          val add = buf2.last+1+total
          buf = buf ++ buf2.map((_,add))
        }
        buf.toArray
      }
    }

    // join matrix entries
    val populatedEntries = emptyEntries
      .groupBy(x => (x.i,x.j))
      .map(x => x._2.reduce((a,b) => new MatrixEntry(a.i,b.j,a.value+b.value)))

    //find gap sequences
    val gapInfo = gapsC.groupBy(_._1).mapValues(x => x.map(_._2))
      .mapValues(x => subseq(x))
      .map(x => x._2.map{y =>
        val value = populatedEntries.filter(i => i.i == (x._1-1) && i.j == y._2).take(1)
        new MatrixEntry((x._1-1).toLong,y._1.toLong,if(value.isEmpty) 0 else value.head.value)})
      .flatten.toSeq

    val gapInfoPar = spark.sparkContext.parallelize(gapInfo)

    val entriesWithGaps = populatedEntries.union(gapInfoPar)

    val coordMat: CoordinateMatrix = new CoordinateMatrix(entriesWithGaps)
    val blocks: BlockMatrix = coordMat.toBlockMatrix(coordMat.numRows.toInt,numBlocks).cache()

    val scoreBlocks = blocks.blocks.map{b =>

      //val doubles = b._2.toArray
      //val max = doubles.reduceLeft(_ max _)
      //Not working here
      //for (i<- 0 to doubles.size-1) println("x:"+b._1._2+" y:"+b._1._1+" i:"+i+ "d:"+doubles(i)+" max:"+max)
      //b._2.transpose.multiply(Vectors.dense(ones))
      new Tuple2(b._1, b._2)
    }

   scoreBlocks.foreach(smb => {
      val cols = smb._2.numCols
      val rows = smb._2.numRows
      val n = (cols) * (rows)
      val block=smb._2
      //For testing

      //val n = (cols)*(rows)
      if (kp > rows) {
        println("_____________________Maximum amount of jumps can not exceed the amount of total rows (" + rows + ")!_________________________")
        System.exit(0)
      }

      //TODO: Count sums of each row at each position from left to right
      val starttime = Calendar.getInstance.getTimeInMillis

      val sums = ArrayBuffer.empty[Array[Long]]

      for (j <- 0 to rows - 1) {
        var rowsum = ArrayBuffer.empty[Long]
        var sum = 0.longValue()
        for (i <- 0 to cols - 1) {
          sum = sum+block.apply(j,i).longValue()
          rowsum.+=(sum)
        }
        sums.+=(rowsum.toArray)
      }

      val heaviest_edges = ArrayBuffer.empty[Array[Tuple2[Int, Long]]]
      //find edges with heaviest shifts
      for (j <- 0 to sums.size - 1) {
        var edgerow = ArrayBuffer.empty[Tuple2[Int, Long]]
        for (i <- 0 to sums(0).size - 1) {
          val sum_ji = sums(j)(i)
          var max_shift_weight: Long = 0
          var max_row = 0
          for (n <- 0 to sums.size - 1) {
            val weight_i_to_end = sums(n)(sums(n).size - 1) - sums(n)(i)
            val weight_with_shift_to_n = weight_i_to_end + sum_ji
            if (max_shift_weight < weight_with_shift_to_n) {
              max_shift_weight = weight_with_shift_to_n
              max_row = n
            }
          }
          edgerow.+=(Tuple2(max_row, max_shift_weight))
        }
        heaviest_edges.+=(edgerow.toArray)
      }
      //printmat3(heaviest_edges.toArray)
      // () -> () -> () -> ()
      /* println("_____________________HEAVIEST EDGES INITIAL_________________________")
      for(j<-0 to heaviest_edges.size-1){
        println("")
        for(i<-0 to heaviest_edges(j).size-1)
            print(" ("+heaviest_edges(j)(i)._1+","+heaviest_edges(j)(i)._2+") ")
      }*/

      //TODO:count path sum for each path starting at index j,0 by following jump index
      //Finds only heaviest path with arbitrary amounts of jumps
      /* for(j<-0 to heaviest_edges.size-1){

        var next = heaviest_edges(j)(0)
        println("PATH")
        print(" ("+j+","+0+") ")
        var path_sum = sums(j)(0)
        for(i<-0 to heaviest_edges(0).size-1){
          path_sum += next._2
          next = heaviest_edges(next._1)(i)
          print(" ("+next._1+","+i+") ")
        }
        println("")
        println(" path sum:"+path_sum)
      }*/


      //TODO: take max shift position for each row, this does that only one shift position per row is recorded!
      //TODO: take multiple maxs and drop out shifts in same position(take only one max in current position)
      //MAX SHIFT PER ROW,
      //Tuple4[Int, Int, Int, Long] stores [src row, dst row, dst col, score]

      var max_shifts = ArrayBuffer.empty[Tuple4[Int, Int, Int, Long]]
      for(j<-0 to heaviest_edges.size-1){
        var max_shift_edge: Tuple4[Int, Int, Int, Long] = Tuple4(0,0,0,0)
        for(i<-0 to heaviest_edges(j).size-1){
          if(max_shift_edge._4 < heaviest_edges(j)(i)._2){
            //discard jumps to same position if found from some other row, take only highest scoring jump
            if(!hasPositionScore(i+1, heaviest_edges(j)(i)._2, max_shifts))
              max_shift_edge = Tuple4(j, heaviest_edges(j)(i)._1, i+1 , heaviest_edges(j)(i)._2)
          }
        }
        max_shifts.+=(max_shift_edge)
      }


      /*println("_____________________MAX SHIFT POSITIONS PER ROW_________________________")
      for(j<-0 to max_shifts.size-1){
          println(" from row "+j+" to position ("+max_shifts(j)._2+","+max_shifts(j)._3+") scores "+max_shifts(j)._4+")")
      }*/

      //TODO: sort by highest score, take k highest shift values and sort by column index
      val sorted_shifts = max_shifts.sortWith(_._4 > _._4)
      var k_highest = ArrayBuffer.empty[Tuple4[Int, Int, Int, Long]]

      for(i <- 0 to kp-1)
        k_highest.+=(sorted_shifts(i))
      val shift_positions = k_highest.sortWith(_._3 < _._3)

      println("")
      println("_____________________K-HIGHEST SHIFT POSITIONS COLUMN SORTED (includes start and end positions)_________________________")
      //for(k <- 0 to shift_positions.size-1)
      //println("shift from row "+shift_positions(k)._1+ " to (row:"+shift_positions(k)._2+",col:"+shift_positions(k)._3+" scores:"+shift_positions(k)._4+")")

      //Add start node to start of array(same as first maxs source row and col is 0)
      val head = Tuple4(shift_positions(0)._1, shift_positions(0)._1, 0 , heaviest_edges(shift_positions(0)._1)(0)._2)
      val last = shift_positions(shift_positions.size-1)
      val tail = Tuple4(last._2, last._2, cols-1 , heaviest_edges(last._2)(heaviest_edges.size-1)._2)
      shift_positions.+=:(head)
      shift_positions.+=(tail)
      //shift_positions.sortWith(_._3 < _._3)



      //shift_positions.+=(tail)
      //TODO: trace path with jumps at k-highest positions
      //Start from row stored in first shift position src row
      var total = 0.longValue()
      var path = ArrayBuffer.empty[Tuple2[Int, Int]]


      //TODO:what if optimum path requires jumping to/from same row multiple times, we need to take k-max from each row and test all paths?
      //TODO: check scores, more jumps should give better score
      //TODO:count sum after next jump to next position, if smaller don't jump
      // TODO: find max scoring jump positions between adjacent shift_positions, use sum table to count wit findHeaviestEdgesBetweenRows()

    //2 jumps, best positions are 19 and 85

    //shift from row 15 to (row:6,col:79 scores:17339)
    //shift from row 18 to (row:6,col:85 scores:17627)

    var col = cols-1
      for(k <- shift_positions.size-1 to 0 by -1){
        val current = shift_positions(k)
        //[src row, dst row, dst col, score]

        var end = 0
        var next: Tuple4[Int, Int, Int, Long] = Tuple4(0,0,0,0)
        var next2: Tuple4[Int, Int, Int, Long] = Tuple4(0,0,0,0)
        if(k>0){
          next = shift_positions(k-1)
        }
        if(k>1){
          next2 = shift_positions(k-2)
        }
        //TODO: some optimization still needed: two errors in individ positions,first or last index missing, k=1,2 and 3 gives correct, k=4 should give 17633, MAX score should be 17638
        val optimal = findHeaviestEdgesBetweenRows(next._1,current._1, next2._3, current._3, sums)
         //we progress to next "local" optimal shift position between two "global" shift positions
        while(col > optimal._1){
          total += block.apply(current._1,col).longValue()
          path.+=(Tuple2(current._1, col))
          col-=1
        }
      }

       println("")
      //println("_____________________HEAVIEST PATH WITH "+kp+"-jumps_________________________")
      //path.reverse.foreach(p => print(p._1+"-"))
      //println("SUM:"+total)

      //println("Running time:"+(Calendar.getInstance.getTimeInMillis-starttime))
      var fos: FSDataOutputStream = null
      val fis = FileSystem.get(new URI("hdfs://node-1:8020"),new Configuration())

      try {

        fos = fis.create(new Path(out+"/"+smb._1._1+"_"+smb._1._2))
        fos.writeBytes(path.toString());
      } catch {
        case e: IOException =>
        //e.printStackTrace()
      }
      fos.close()

      //println(adj.numNonzeros)

    //TODO: combine blocks to create final sequence
    //val vectsums = indScoreMat.multiply(Matrices.ones(30,1))
    })
    //pathblocks.saveAsTextFile(out)

    spark.stop()
  }

  def printmat3(dynmat: Array[Array[(Int, Long)]]) = {
    println("_____________________MATRIX_________________________")
    for(j<-0 to dynmat.size-1){
      println("")
      for(i<-0 to dynmat(0).size-1)
        print(" ("+dynmat(j)(i)._1+","+dynmat(j)(i)._2+")")
    }
    println(" ")
  }

  def findHeaviestEdgesBetweenColumns(start: Int, end: Int, sums:  ArrayBuffer[Array[Long]]): ArrayBuffer[Array[(Int, Long)]] = {
    val heaviest_edges = ArrayBuffer.empty[Array[Tuple2[Int, Long]]]
    //find edges with heaviest shifts between positions
    for(j<-0 to sums.size-1){
      var edgerow = ArrayBuffer.empty[Tuple2[Int, Long]]
      for(i<-start to end){
        val sum_ji = sums(j)(i)
        var max_shift_weight : Long = 0
        var max_row = 0
        for(n<-0 to sums.size-1){
          val weight_i_to_end = sums(n)(sums(n).size-1) - sums(n)(i)
          val weight_with_shift_to_n = weight_i_to_end + sum_ji
          if(max_shift_weight < weight_with_shift_to_n){
            max_shift_weight = weight_with_shift_to_n
            max_row = n
          }
        }
        edgerow.+=(Tuple2(max_row, max_shift_weight))
      }
      heaviest_edges.+=(edgerow.toArray)
    }
    return heaviest_edges
  }
  def findHeaviestEdgesBetweenRows(src: Int, dst: Int, start: Int, end: Int, sums:  ArrayBuffer[Array[Long]]): (Int, Long) = {
    val heaviest_edges = ArrayBuffer.empty[Array[Tuple2[Int, Long]]]
    //find edges with heaviest shifts between positions

      var max_pos = 0
      var max_shift_weight : Long = 0
      for(i<-start to end){
          val sum_ji = sums(src)(i)
          val weight_i_to_end = sums(dst)(sums(dst).size-1) - sums(dst)(i)
          val weight_with_shift_to_n = weight_i_to_end + sum_ji
          if(max_shift_weight < weight_with_shift_to_n){
            max_shift_weight = weight_with_shift_to_n
            max_pos = i
          }
      }

    return Tuple2(max_pos, max_shift_weight)
  }

  def hasPosition(pos: Int, max_shifts: ArrayBuffer[(Int, Int, Int, Long)]): Boolean ={

    for(i<-0 to max_shifts.size-1)
      if(max_shifts(i)._3==pos)
        return true
    return false

  }
  def hasPositionScore(pos: Int, score: Long, max_shifts: ArrayBuffer[(Int, Int, Int, Long)]): Boolean ={

    for(i<-0 to max_shifts.size-1)
      if(max_shifts(i)._3==pos || max_shifts(i)._4==score)
        return true
    return false

  }
}
